---
title: "Loan Prediction"
author: "Aditya Sinha, Alex von Schwerdtner, Fusu Luo, Jennifer Horita , Weijia Suo"
output: html_notebook
---

```{r}

install.packages(c("data.table", "ggplot2", "ggthemes", "scales", "ISLR", "randomForest", "caret", "rpart.plot", "rpart", "caTools", "ROCR"))

```

```{r}
library(data.table)
library(ggplot2)
library(ggthemes)
library(scales)
library(ISLR)
```

```{r}
dd <- fread("./Training-Data.csv")
head(dd,n=10)
```

---
EDA: Income
---

```{r}
print(paste0("Minimum salary: ",min(dd$Income)))
print(paste0("Maximum salary: ",max(dd$Income)))
```

```{r}
salary_distribution <- ggplot(dd, aes(dd$Income, fill=..count..))+
  geom_histogram(binwidth=1000000) + 
  labs(x="Income", y="Number") +
  ggtitle("Frequency Distribution of Income")
  
salary_distribution
```

---
EDA: Age
---

```{r}
age_distribution <- ggplot(dd, aes(dd$Age, fill=..count..))+
  geom_histogram(binwidth = 10,) + 
  labs(x="Age", y="Number") +
  ggtitle("Frequency Distribution of Age")+
  scale_x_continuous(breaks = seq(0,250,25))
  
age_distribution
```

---
EDA: House Ownership
---

```{r}
rented_vs_owned <- dd[, .(count = .N), by = House_Ownership]
print(rented_vs_owned)
```

```{r}
ggplot(rented_vs_owned, aes (x="", y = count, fill = House_Ownership)) + 
  geom_col(position = 'stack', width = 1) +
  geom_text(aes(label = paste0(round(count / sum(count) * 100), "%"), x = 1.3),
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Blues")+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "House Ownership",
       x = NULL,
       y = NULL,
       title = "House Ownership") + 
  coord_polar("y")

```

---
EDA: Car Ownership
---

```{r}
car_ownership <- dd[, .(count = .N), by = Car_Ownership]
print(car_ownership)
```

```{r}
ggplot(car_ownership, aes (x="", y = count, fill = Car_Ownership)) + 
  geom_col(position = 'stack', width = 1) +
  geom_text(aes(label = paste0(round(count / sum(count) * 100), "%"), x = 1.3),
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Blues")+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "Car Ownership",
       x = NULL,
       y = NULL,
       title = "Car Ownership") + 
  coord_polar("y")

```

---
EDA: Marital Status
---

```{r}
marital_status <- dd[, .(count = .N), by = `Married/Single`]
print(marital_status)
```

```{r}
ggplot(marital_status, aes (x="", y = count, fill = `Married/Single`)) + 
  geom_col(position = 'stack', width = 1) +
  geom_text(aes(label = paste0(round(count / sum(count) * 100), "%"), x = 1.3),
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Blues")+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "Marital Status",
       x = NULL,
       y = NULL,
       title = "Marital Status") + 
  coord_polar("y")

```

---
EDA: Profession
---

```{r}
profession <- dd[, .(count = .N), by = `Profession`][order(-rank(count))]
print(nrow(profession))
```
```{r}

print(paste0("Disitinct Profession:", nrow(profession)))

```

```{r}

ggplot(data=head(profession,n=10), aes(x=reorder(Profession, count), y=count)) +
  geom_bar(stat="identity")+
  coord_flip()+
  labs(title = 'Top 10 Profession') +
  xlab("Profession")

```

---
Machine learning
---

---
Helper Method
---

```{r}
mse = function(x,y) { mean((x-y)^2)}

```

---
Data Cleaning
---

```{r}
library(data.table)
library(ggplot2)
library(ggthemes)
library(glmnet)
library(dplyr)
theme_set(theme_bw())
```

```{r}
data <- dd

#REMOVE unwanted ID
data$Id <- NULL

#Change col names
colnames(data)[4] <- "Marital_Status"

str(data)
```
```{r}
summary(data)
```

```{r}

set.seed(123)
smp_size <- floor(0.70 * nrow(data))
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

x_data <- model.matrix( ~ -1 + Income + Age +
 Experience + Marital_Status +
 House_Ownership + Car_Ownership + CITY + STATE + Profession + CURRENT_JOB_YRS + CURRENT_HOUSE_YRS, data)

# outcome is median house value in millions
y_data <- dd$Risk_Flag

x_train <- x_data[train_ind, ]
y_train <- y_data[train_ind]
x_test <- x_data[-train_ind, ]
y_test <- y_data[-train_ind]

```

---
Lasso Regression
---

```{r}

lasso_model <- cv.glmnet(x_train, y=y_train, alpha = 1, nfolds = 5)
best_lasso_lambda <- lasso_model$lambda.min


best_model_lasso <- glmnet(x_train, y_train, alpha = 1, lambda = best_lasso_lambda)
#coef(best_model)

predictions_train_lasso <- predict(best_model_lasso, s = best_lasso_lambda, newx = x_train)
predictions_test_lasso <- predict(best_model_lasso, s = best_lasso_lambda, newx = x_test)

mse_test_lasso = (mse(predictions_test_lasso, y_test))

print(mse_test_lasso)

```

---
Ridge Regression
---



```{r}

ridge_model <- cv.glmnet(x_train, y=y_train, alpha = 0, nfolds = 5)
best_ridge_lambda <- ridge_model$lambda.min


best_model_ridge <- glmnet(x_train, y_train, alpha = 0, lambda = best_ridge_lambda)
#coef(best_model_ridge)

predictions_train_ridge <- predict(best_model_ridge, s = best_ridge_lambda, newx = x_train)
predictions_test_ridge <- predict(best_model_ridge, s = best_ridge_lambda, newx = x_test)

mse_test_ridge = (mse(predictions_test_ridge, y_test))

print(mse_test_ridge)

```

---
Linear Regression
---

```{r}

x_train_df <- as.data.frame(x_train)
linear_model = lm(y_train ~. + 0 , data=x_train_df)

predictions_test_linear_df <- predict(linear_model, x_test_df)
predictions_test_linear <- as.matrix(predictions_test_linear_df)

mse_test_lm1 <- mse(y_test, predictions_test_linear)

print(mse.test.lm1)

```

---
Logistic regression
---

```{r}
library(caTools)
library(ROCR)

log_model <- glm(y_train ~ ., data=x_train_df)

predictions_test_log_df <- predict(log_model, x_test_df, type = "response")
predictions_test_log <- as.matrix(predictions_test_log_df)

mse_test_log <- mse(y_test, predictions_test_log_df)

print(mse_test_log)


```

---
Re-sampling
---

```{r}
## Make sample dataset-1: Based on original dataset ratio
# Get Risk_Flag original dataset ratio
total <- length(data$Risk_Flag)
true <- length(data$Risk_Flag[data$Risk_Flag==1])
false <- length(data$Risk_Flag[data$Risk_Flag==0])

# Calculate dataset ratio
true_ratio <- true/total
false_ratio <- false/total
# Get sample dataset size for 0 and 1
true_smp_size <- floor(true_ratio * 20000)
false_smp_size <- floor(false_ratio * 20000)
# set seed
set.seed(12345)
# Split dataset for Risk_Flag=1 and Risk_Flag=0
true_ind <- sample(seq_len(nrow(data[data$Risk_Flag==1])), size = true_smp_size)
false_ind <- sample(seq_len(nrow(data[data$Risk_Flag==0])), size = false_smp_size)
# Split the data frames
true_smp <- (data[data$Risk_Flag==1])[true_ind, ]
false_smp <- (data[data$Risk_Flag==0])[false_ind, ]
# Create sample dataset size
sample1 <- rbind(false_smp, true_smp)
# Randomize sample's order
sample1 <- sample1[sample(nrow(sample1)),]

## Split sample dataset to test and train
# Split dataset
set.seed(123)
smp_size <- floor(0.70 * nrow(sample1))
train_ind <- sample(1:nrow(sample1),size = smp_size)

# Split the data frames
risk_train <- sample1[train_ind,]
risk_test <- sample1[-train_ind,]
# Get sample output 
y_train_sample <- risk_train$Risk_Flag
y_test_sample <- risk_test$Risk_Flag
# Check test and train dataset
prop.table(table(risk_train$Risk_Flag))
prop.table(table(risk_test$Risk_Flag))

```

---
Random Forest
---

```{r}
library(randomForest)
library(caret)

# Convert numerical variable to factor
risk_train$Risk_Flag <- as.factor(risk_train$Risk_Flag)
risk_test$Risk_Flag <- as.factor(risk_test$Risk_Flag)
# Run randomForest model
risk_rf <- randomForest(Risk_Flag ~ ., data = risk_train)
risk_rf_pred <- predict(risk_rf, risk_test)
# Check confusion Matrix
confMat_rf <- confusionMatrix(risk_rf_pred,risk_test$Risk_Flag)
confMat_rf
# Calculate MSE for random forest
mse.forest <- mean(((as.integer(risk_rf_pred)-1) - y_test_sample) ^ 2)
print(mse.forest)


```

---
Decision Tree
---

```{r}
# machine learning - decision tree
#install.packages(c("rpart.plot", "rpart"))
library(rpart)
library(rpart.plot)

tree.model <-rpart(Risk_Flag~Income + Age +
                     Experience + Marital_Status +
                     House_Ownership + Car_Ownership +  CURRENT_JOB_YRS + Profession + CITY + STATE + CURRENT_HOUSE_YRS
, data = risk_train,control=rpart.control(cp=0.003))
tree.model
summary(tree.model)

plot(tree.model,compress=TRUE)
rpart.plot(tree.model)
text(tree.model,pretty = 0 )

```


