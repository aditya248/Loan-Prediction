---
title: "Loan Prediction"
author: "Aditya Sinha, Alex von Schwerdtner, Fusu Luo, Jennifer Horita , Weijia Suo"
output: html_notebook
---


# Executive Summary


# Problem Definition

Build machine learning models to predict who are possible defaulters for the consumer loans product based on historic customer behavior and their background information such as income, age, professional experience, profession, whether married or single and etc.

# Data Source

*Dataset from Kaggle*
Provided by Univ.AI for a Hackathon
**Source:** https://www.kaggle.com/subhamjain/loan-prediction-based-on-customer-behavior?select=Training+Data.csv

# Setup

## Installing Packages
```{r}
install.packages(c("data.table", "ggplot2", "ggthemes", "scales", "ISLR", "randomForest", "caret", "rpart.plot", "rpart", "caTools", "ROCR", "gbm","SmartEDA"))
```

## Loading Packages to Library
```{r}
library(data.table)
library(ggplot2)
library(ggthemes)
library(scales)
library(ISLR)
library(glmnet)
library(dplyr)
library(randomForest)
library(caret)
library(gbm)
library(caTools)
library(ROCR)
library(rpart)
library(rpart.plot)
library(SmartEDA)

theme_set(theme_bw())
```

## Loading dataset into the environment
```{r}
data <- fread("./Training-Data.csv")
head(data,n=10)
```

```{r}
str(data)
```

# Re-sampling

```{r}

## Make sample dataset-1: Based on original dataset ratio
# Get Risk_Flag original dataset ratio
total <- length(data$Risk_Flag)
true <- length(data$Risk_Flag[data$Risk_Flag==1])
false <- length(data$Risk_Flag[data$Risk_Flag==0])

# Calculate dataset ratio
true_ratio <- true/total
false_ratio <- false/total
# Get sample dataset size for 0 and 1
true_smp_size <- floor(true_ratio * 20000)
false_smp_size <- floor(false_ratio * 20000)
# set seed
set.seed(123)
# Split dataset for Risk_Flag=1 and Risk_Flag=0
true_ind <- sample(seq_len(nrow(data[data$Risk_Flag==1])), size = true_smp_size)
false_ind <- sample(seq_len(nrow(data[data$Risk_Flag==0])), size = false_smp_size)
# Split the data frames
true_smp <- (data[data$Risk_Flag==1])[true_ind, ]
false_smp <- (data[data$Risk_Flag==0])[false_ind, ]
# Create sample dataset size
sample1 <- rbind(false_smp, true_smp)
# Randomize sample's order
sample1 <- sample1[sample(nrow(sample1)),]

data <- sample1
str(data)
```


# Data Cleaning

```{r}

data$CITY <- as.data.frame(gsub("[[:punct:]]", "", as.matrix(data$CITY)))
data$STATE <- as.data.frame(gsub("[[:punct:]]", "", as.matrix(data$STATE))) 

#REMOVE unwanted ID
data$Id <- NULL

#Change col names
colnames(data)[4] <- "Marital_Status"

str(data)
```
```{r}
summary(data)
```


# Exploratory Data Analysis

## General Analysis - Overview

```{r}
# Overview of the data - Type = 1
ExpData(data=data,type=1)

# Structure of the data - Type = 2
ExpData(data=data,type=2, fun = c("mean", "median", "var"))
```

## EDA: Income

```{r}
print(paste0("Minimum salary: ",min(data$Income)))
print(paste0("Maximum salary: ",max(data$Income)))
```

```{r}
salary_distribution <- ggplot(data, aes(data$Income, fill=..count..))+
  geom_histogram(binwidth=1000000) + 
  labs(x="Income", y="Number") +
  ggtitle("Frequency Distribution of Income")
  
salary_distribution
```

## EDA: Age

```{r}
age_distribution <- ggplot(data, aes(data$Age, fill=..count..))+
  geom_histogram(binwidth = 10,) + 
  labs(x="Age", y="Number") +
  ggtitle("Frequency Distribution of Age")+
  scale_x_continuous(breaks = seq(0,250,25))
  
age_distribution
```

## EDA: House Ownership

```{r}
rented_vs_owned <- data[, .(count = .N), by = House_Ownership]
print(rented_vs_owned)
```

```{r}
ggplot(rented_vs_owned, aes (x="", y = count, fill = House_Ownership)) + 
  geom_col(position = 'stack', width = 1) +
  geom_text(aes(label = paste0(round(count / sum(count) * 100), "%"), x = 1.3),
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Blues")+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "House Ownership",
       x = NULL,
       y = NULL,
       title = "House Ownership") + 
  coord_polar("y")

```

## EDA: Car Ownership

```{r}
car_ownership <- data[, .(count = .N), by = Car_Ownership]
print(car_ownership)
```

```{r}
ggplot(car_ownership, aes (x="", y = count, fill = Car_Ownership)) + 
  geom_col(position = 'stack', width = 1) +
  geom_text(aes(label = paste0(round(count / sum(count) * 100), "%"), x = 1.3),
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Blues")+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "Car Ownership",
       x = NULL,
       y = NULL,
       title = "Car Ownership") + 
  coord_polar("y")

```

## EDA: Marital Status

```{r}
marital_status <- data[, .(count = .N), by = `Marital_Status`]
print(marital_status)
```

```{r}
ggplot(marital_status, aes (x="", y = count, fill = `Marital_Status`)) + 
  geom_col(position = 'stack', width = 1) +
  geom_text(aes(label = paste0(round(count / sum(count) * 100), "%"), x = 1.3),
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Blues")+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "Marital Status",
       x = NULL,
       y = NULL,
       title = "Marital Status") + 
  coord_polar("y")

```

## EDA: Profession

```{r}
profession <- data[, .(count = .N), by = `Profession`][order(-rank(count))]
print(paste0("Disitinct Profession:", nrow(profession)))
```

```{r}

ggplot(data=head(profession,n=10), aes(x=reorder(Profession, count), y=count)) +
  geom_bar(stat="identity")+
  coord_flip()+
  labs(title = 'Top 10 Profession') +
  xlab("Profession")

```

## EDA: Target Variable: "Risk_Flag"

```{r}
#Cross tabulation with target variable
ExpCTable(data,Target="Risk_Flag",margin=1,clim=10,nlim=3,round=2,bin=NULL,per=F)
```
```{r}
#Distributions of Numerical variables
plot1 <- ExpNumViz(data,target="Risk_Flag",type=1,nlim=5,fname=NULL,col=c("darkgreen","springgreen3","springgreen1"),Page=c(2,2),sample=5)
plot1[[1]]
```

```{r}
#Distributions of categorical variables
plot2 <- ExpCatViz(data,target="Risk_Flag",fname=NULL,clim=15,col=c("slateblue4","slateblue1"),margin=2,Page = c(1,1),sample=5)
plot2[[1]]
```
# Supervised-Machine Learning Models

## Helper Method

```{r}
mse = function(x,y) { mean((x-y)^2)}
```

## Train-Test Split

```{r}

set.seed(123)
smp_size <- floor(0.70 * nrow(data))
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

x_data <- model.matrix( ~ -1 + Income + Age +
 Experience + Marital_Status +
 House_Ownership + Car_Ownership + CITY + STATE + Profession + CURRENT_JOB_YRS + CURRENT_HOUSE_YRS, data)

# outcome is median house value in millions
y_data <- data$Risk_Flag

x_train <- x_data[train_ind, ]
y_train <- y_data[train_ind]
x_test <- x_data[-train_ind, ]
y_test <- y_data[-train_ind]

x_train_df <- as.data.frame(x_train)
x_test_df <- as.data.frame(x_test)


# categorical train test split
x_rf_train <- data[train_ind, ]
x_rf_test <- data[-train_ind]

y_rf_train_sample <- x_rf_train$Risk_Flag
y_rf_test_sample <- x_rf_test$Risk_Flag


# Check test and train dataset
prop.table(table(y_train))
prop.table(table(y_test))

```

## Linear Regression

```{r}

linear_model = lm(y_train ~. + 0 , data=x_train_df)

predictions_test_linear_df <- predict(linear_model, x_test_df)
predictions_test_linear <- as.matrix(predictions_test_linear_df)

mse_test_lm1 <- mse(y_test, predictions_test_linear)

print(mse_test_lm1)

```

## Ridge Regression

```{r}

ridge_model <- cv.glmnet(x_train, y=y_train, alpha = 0, nfolds = 5)
best_ridge_lambda <- ridge_model$lambda.min


best_model_ridge <- glmnet(x_train, y_train, alpha = 0, lambda = best_ridge_lambda)
#coef(best_model_ridge)

predictions_train_ridge <- predict(best_model_ridge, s = best_ridge_lambda, newx = x_train)
predictions_test_ridge <- predict(best_model_ridge, s = best_ridge_lambda, newx = x_test)

mse_test_ridge = (mse(predictions_test_ridge, y_test))

print(mse_test_ridge)

```

## Lasso Regression

```{r}

lasso_model <- cv.glmnet(x_train, y=y_train, alpha = 1, nfolds = 5)
best_lasso_lambda <- lasso_model$lambda.min


best_model_lasso <- glmnet(x_train, y_train, alpha = 1, lambda = best_lasso_lambda)
#coef(best_model)

predictions_train_lasso <- predict(best_model_lasso, s = best_lasso_lambda, newx = x_train)
predictions_test_lasso <- predict(best_model_lasso, s = best_lasso_lambda, newx = x_test)

mse_test_lasso = (mse(predictions_test_lasso, y_test))

print(mse_test_lasso)

```

## Logistic regression

```{r}

log_model <- glm(y_train ~ ., data=x_train_df, family = "binomial")

predictions_test_log_df <- predict(log_model, x_test_df, type = "response")
predictions_test_log <- as.matrix(predictions_test_log_df)

mse_test_log <- mse(y_test, predictions_test_log_df)

print(mse_test_log)


```

## Decision Tree

```{r}
# machine learning - decision tree

tree_model <-rpart(Risk_Flag ~ ., data = x_rf_train, control=rpart.control(cp=0.003))
yhat4.tree <- predict(tree_model, x_rf_test)

mse_test_dtress <- mse(y_rf_test_sample, yhat4.tree)
print(mse_test_dtress)

```

```{r}
rpart.plot(tree_model, type = 1)

```

## Random Forest

```{r}

# Run randomForest model
risk_rf <- randomForest(Risk_Flag ~ ., data = x_rf_train, importance=TRUE)
risk_rf_pred <- predict(risk_rf, x_rf_test)

# Calculate MSE for random forest
mse_test_forest <- mse(y_rf_test_sample, risk_rf_pred)
print(mse_test_forest)

```


## Generalized Boosted Regression 

```{r}

fit.btree <- gbm(y_train ~.,
data = x_train_df,
distribution = "gaussian",
n.trees = 200,
interaction.depth = 2,
shrinkage = 0.001)

yhat.btree <- predict(fit.btree, x_test_df, n.trees = 100)
predictions_test_xg <- as.matrix(yhat.btree)

mse_test_gbmtress <- mse(y_test , predictions_test_xg)
print(mse_test_gbmtress)

```


# Conclusion
## Compare all model MSE
```{r}

# Create the data for the chart
MSE <- c(mse_test_forest, mse_test_ridge, mse_test_lasso, mse_test_lm1, mse_test_log, mse_test_dtress, mse_test_gbmtress)
Model <- c("Random Forest","Ridge Regression","Lasso Regression","Linear Regression","Logistic Regression","Decision Tree", "Generalized Boosted Regression")

mse.data <- data.frame(Model,MSE)
mse.data

```


## Get important variables
```{r}
importance(risk_rf)
varImpPlot(risk_rf)

## Higher the value of mean decrease accuracy or mean decrease gini score , higher the importance of the variable in the model. In the plot shown above, Income is most important variable.
```













